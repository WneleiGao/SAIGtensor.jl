{
    "docs": [
        {
            "location": "/", 
            "text": "SAIGtensor Documentation\n\n\nTensor tools for seismic data processing\n\n\n\n\nDense Tensor\n\n\n\n\nCP Tensor\n\n\n\n\nIndex\n\n\n\n\nSAIGtensor.KRP\n\n\nSAIGtensor.cp2tensor\n\n\nSAIGtensor.cp_gradient\n\n\nSAIGtensor.cpnorm\n\n\nSAIGtensor.cpnormalize!\n\n\nSAIGtensor.cptLeft\n\n\nSAIGtensor.cptRight\n\n\nSAIGtensor.cptfirst\n\n\nSAIGtensor.fitness\n\n\nSAIGtensor.formXst!\n\n\nSAIGtensor.formZs!\n\n\nSAIGtensor.initCptensor\n\n\nSAIGtensor.initTensor\n\n\nSAIGtensor.matricization\n\n\nSAIGtensor.randSpl!\n\n\nSAIGtensor.recursiveKRP\n\n\nSAIGtensor.sidx2sub\n\n\nSAIGtensor.tnorm\n\n\nSAIGtensor.ttf\n\n\nSAIGtensor.ttf_slow\n\n\nSAIGtensor.ttv\n\n\nSAIGtensor.unmatricization\n\n\nSAIGtensor.updateAn!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#saigtensor-documentation", 
            "text": "Tensor tools for seismic data processing", 
            "title": "SAIGtensor Documentation"
        }, 
        {
            "location": "/#dense-tensor", 
            "text": "", 
            "title": "Dense Tensor"
        }, 
        {
            "location": "/#cp-tensor", 
            "text": "", 
            "title": "CP Tensor"
        }, 
        {
            "location": "/#index", 
            "text": "SAIGtensor.KRP  SAIGtensor.cp2tensor  SAIGtensor.cp_gradient  SAIGtensor.cpnorm  SAIGtensor.cpnormalize!  SAIGtensor.cptLeft  SAIGtensor.cptRight  SAIGtensor.cptfirst  SAIGtensor.fitness  SAIGtensor.formXst!  SAIGtensor.formZs!  SAIGtensor.initCptensor  SAIGtensor.initTensor  SAIGtensor.matricization  SAIGtensor.randSpl!  SAIGtensor.recursiveKRP  SAIGtensor.sidx2sub  SAIGtensor.tnorm  SAIGtensor.ttf  SAIGtensor.ttf_slow  SAIGtensor.ttv  SAIGtensor.unmatricization  SAIGtensor.updateAn!", 
            "title": "Index"
        }, 
        {
            "location": "/denseTensor/page1/", 
            "text": "Dense Tensor\n\n\nSome operations with respect to basic dense tensor, I need to add some latex equations to my document\n\n\n\n\nFunctions\n\n\n#\n\n\nSAIGtensor.initTensor\n \n \nFunction\n.\n\n\nInitTensor(N, I, D)\n\n\n\n\ncreate a tensor object\n\n\nArguments\n\n\n\n\nN\n: Int64, dimensions\n\n\nI\n: Array{Int64}, size of each dimension\n\n\nD\n: Array{Float64}, multi-dimension array\n\n\n\n\nReturns\n\n\n\n\nX\n: tensor object\n\n\n\n\nExample\n\n\njulia\n N = 5; I = [23, 42, 13, 14, 17]; D = rand(I...)\njulia\n X = InitTensor(N, I, D)\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.matricization\n \n \nFunction\n.\n\n\nXn = matricization(X::tensor, n::Int64)\n\n\n\n\nunfolding tensor along n dimension\n\n\nArguments\n\n\n\n\nX\n: tensor, tensor object\n\n\nn\n: Int64, unfolding along n dimension, 1\n= n \n= N\n\n\n\n\nReturns\n\n\n\n\nXn\n: Array{Float64,2}, matrix\n\n\n\n\nExample\n\n\njulia\n N = 3; I = [3, 4, 2]; D = reshape(collect(1:prod(I)), I...)\njulia\n X = initTensor(N, I, D)\njulia\n n = 3\njulia\n Xn = matricization(X, n)\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.unmatricization\n \n \nFunction\n.\n\n\nX = unmatricization(Xn, n, I)\n\n\nfolding an Array back to tensor\n\n\nArguments\n\n\n\n\nXn\n: Array{Float64}, the array to be folded\n\n\nn\n: Int64, folding along n dimension, 1\n= n \n= N\n\n\nI\n: Array{Int64}, size of tensor\n\n\n\n\nReturns\n\n\n\n\nX\n: tensor\n\n\n\n\nExample\n\n\njulia\n N = 3; I = [3, 4, 2]; D = reshape(collect(1:prod(I)), I[1], prod(I[2:N]))\njulia\n n = 1; X = unmatricization(D, n, I)\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.KRP\n \n \nFunction\n.\n\n\nr = KRP(a1, a2)\n\n\n\n\nKhatri-Rao product a2 \u2299 a1\n\n\nArguments\n\n\n\n\na1\n: Array{Float64}, it can be a 1D or 2D array\n\n\na2\n: Array{Float64}, it can be a 1D or 2D array\n\n\n\n\nReturns\n\n\n\n\nr\n: Array{Float64}, it can be a 1D or 2D array\n\n\n\n\nExample\n\n\njulia\n R = 20; m1 = 77; m2 = 33;\njulia\n a1 = rand(m1,R); a2 = rand(m2,R);\njulia\n r = KRP(a1, a2)\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.recursiveKRP\n \n \nFunction\n.\n\n\nr = recursiveKRP(A)\n\n\n\n\nrecursive Khatri-Rao product A[N]\u2299...\u2299A[1]\n\n\nArguments\n\n\n\n\nA\n: Array{Array{Float64}}, each element can be 1D or 2D array\n\n\n\n\nReturns\n\n\n\n\nr\n: Array{Float64}, it may be a 1D or 2D array\n\n\n\n\nExample\n\n\njulia\n I = [21, 32, 43, 24]; R = 3;\njulia\n N = length(I);\njulia\n A = Array{Array{Float64}}(N)\njulia\n for m = 1 : N\njulia\n     A[m] = rand(I[m], R)\njulia\n end\njulia\n r1 = recursiveKRP(A);\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.tnorm\n \n \nFunction\n.\n\n\nl = tnorm(X)\n\n\n\n\ncompute the frobenius norm of tensor\n\n\nArguments\n\n\n\n\nX\n: tensor\n\n\n\n\nReturns\n\n\n\n\nl\n: the frobenius norm of X\n\n\n\n\nExample\n\n\njulia\n I = [21, 32, 43]; N = 3; D = rand(I...)\njulia\n l = tnorm(tensor(N, I, D))\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.ttv\n \n \nFunction\n.\n\n\ny = ttv(X, v, dims)\n\n\n\n\ncompute tensor times a set of vectors\n\n\nArguments\n\n\n\n\nX\n: tensor\n\n\nv\n: Array{Array{Float64,1}}\n\n\ndims\n: specification the multiplication on which dimension\n\n\n\n\nReturns\n\n\n\n\ny\n: Array{Float64,1}, length is prod(remdims)\n\n\n\n\nsource", 
            "title": "DenseTensor"
        }, 
        {
            "location": "/denseTensor/page1/#dense-tensor", 
            "text": "Some operations with respect to basic dense tensor, I need to add some latex equations to my document", 
            "title": "Dense Tensor"
        }, 
        {
            "location": "/denseTensor/page1/#functions", 
            "text": "#  SAIGtensor.initTensor     Function .  InitTensor(N, I, D)  create a tensor object  Arguments   N : Int64, dimensions  I : Array{Int64}, size of each dimension  D : Array{Float64}, multi-dimension array   Returns   X : tensor object   Example  julia  N = 5; I = [23, 42, 13, 14, 17]; D = rand(I...)\njulia  X = InitTensor(N, I, D)  source  #  SAIGtensor.matricization     Function .  Xn = matricization(X::tensor, n::Int64)  unfolding tensor along n dimension  Arguments   X : tensor, tensor object  n : Int64, unfolding along n dimension, 1 = n  = N   Returns   Xn : Array{Float64,2}, matrix   Example  julia  N = 3; I = [3, 4, 2]; D = reshape(collect(1:prod(I)), I...)\njulia  X = initTensor(N, I, D)\njulia  n = 3\njulia  Xn = matricization(X, n)  source  #  SAIGtensor.unmatricization     Function .  X = unmatricization(Xn, n, I)  folding an Array back to tensor  Arguments   Xn : Array{Float64}, the array to be folded  n : Int64, folding along n dimension, 1 = n  = N  I : Array{Int64}, size of tensor   Returns   X : tensor   Example  julia  N = 3; I = [3, 4, 2]; D = reshape(collect(1:prod(I)), I[1], prod(I[2:N]))\njulia  n = 1; X = unmatricization(D, n, I)  source  #  SAIGtensor.KRP     Function .  r = KRP(a1, a2)  Khatri-Rao product a2 \u2299 a1  Arguments   a1 : Array{Float64}, it can be a 1D or 2D array  a2 : Array{Float64}, it can be a 1D or 2D array   Returns   r : Array{Float64}, it can be a 1D or 2D array   Example  julia  R = 20; m1 = 77; m2 = 33;\njulia  a1 = rand(m1,R); a2 = rand(m2,R);\njulia  r = KRP(a1, a2)  source  #  SAIGtensor.recursiveKRP     Function .  r = recursiveKRP(A)  recursive Khatri-Rao product A[N]\u2299...\u2299A[1]  Arguments   A : Array{Array{Float64}}, each element can be 1D or 2D array   Returns   r : Array{Float64}, it may be a 1D or 2D array   Example  julia  I = [21, 32, 43, 24]; R = 3;\njulia  N = length(I);\njulia  A = Array{Array{Float64}}(N)\njulia  for m = 1 : N\njulia      A[m] = rand(I[m], R)\njulia  end\njulia  r1 = recursiveKRP(A);  source  #  SAIGtensor.tnorm     Function .  l = tnorm(X)  compute the frobenius norm of tensor  Arguments   X : tensor   Returns   l : the frobenius norm of X   Example  julia  I = [21, 32, 43]; N = 3; D = rand(I...)\njulia  l = tnorm(tensor(N, I, D))  source  #  SAIGtensor.ttv     Function .  y = ttv(X, v, dims)  compute tensor times a set of vectors  Arguments   X : tensor  v : Array{Array{Float64,1}}  dims : specification the multiplication on which dimension   Returns   y : Array{Float64,1}, length is prod(remdims)   source", 
            "title": "Functions"
        }, 
        {
            "location": "/CPTensor/page1/", 
            "text": "CP Tensor\n\n\nSome operations with respect to CP decomposition\n\n\n\n\nFunctions\n\n\n#\n\n\nSAIGtensor.initCptensor\n \n \nFunction\n.\n\n\ninitCptensor(N, I, R, lambda, A)\n\n\n\n\ncreate a cptensor object, right to test, it is a great work. fantastic packages.\n\n\nArguments\n\n\n\n\nN\n: Int64, dimensions\n\n\nI\n: Array{Int64}, size of each dimension\n\n\nR\n: Int64, rank of tensor\n\n\nlambda\n: eigenvalue of tensor\n\n\nA\n: Array{Array{Float64,2}}, basis\n\n\n\n\nReturns\n\n\n\n\nX\n: CP tensor\n\n\n\n\nExample\n\n\njulia\n N = 5; I = [23, 42, 13, 14, 17];\njulia\n R = 10; lambda = rand(N);\njulia\n A = Array{Array{Float64,2}}(N)\njulia\n for m = 1 : N\njulia\n     A[m] = rand(I[m], R)\njulia\n end\njulia\n X = initCptensor(N, I, R, lambda, A);\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cpnorm\n \n \nFunction\n.\n\n\nl = cpnorm(X)\n\n\n\n\nefficient way of computing the Frobenius norm of cp tensor\n\n\nArguments\n\n\n\n\nX\n: cptensor\n\n\n\n\nReturns\n\n\n\n\nl\n: the frobenius norm of X\n\n\n\n\nExample\n\n\njulia\n I = [21, 32, 43, 24]; R = 3;\njulia\n N = length(I);\njulia\n A = Array{Array{Float64}}(N)\njulia\n for m = 1 : N\njulia\n     A[m] = rand(I[m], R)\njulia\n end\njulia\n X = initCptensor(N, I, R, lambda, A); l = cpnorm(X);\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.fitness\n \n \nFunction\n.\n\n\nf = fitness(X, Y)\n\n\n\n\ncompute the relative fitness of lowrank cptensor to a full tensor\n\n\nArguments\n\n\n\n\nX\n: cptensor\n\n\nY\n: tensor\n\n\n\n\nReturns\n\n\n\n\nf\n: relative fitness\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cptfirst\n \n \nFunction\n.\n\n\ncptfirst(I)\n\n\n\n\ndecide which factor should be update first, the updating order is ns, ns-1:-1:1, ns+1, ns+2:N\n\n\nArguments\n\n\n\n\nI\n  : Array{Int64,1}, dimensions of tensor\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cptRight\n \n \nFunction\n.\n\n\ncptRight(X, A, n)\n\n\n\n\ncompute X \u00d7 A[n+1] \u00d7 A[n+2] \u00d7 ... \u00d7 A[N]\n\n\nArguments\n\n\n\n\nX\n: tensor\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\nn\n: Int64\n\n\n\n\nReturns\n\n\n\n\nRn\n : Array{Float64,2}\n\n\n\n\nsource\n\n\nWhen applied matrix, using the efficient way to compute them based on the property X \u00d7 A[n+1] \u00d7 A[n+2] \u00d7 ... \u00d7 A[N] = (X \u00d7 A[n+2] \u00d7 ... \u00d7 A[N]) \u00d7 A[n+1]\n\n\n\n\nRnp1\n: Array{Float64,2} with size prod(I[1:n+1]) * R\n\n\nAnp1\n: Array{Array{Float64,2}} with size I[n+1] * R\n\n\nI\n   : Array{Int64}\n\n\nn\n   : Int64\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cptLeft\n \n \nFunction\n.\n\n\ncptLeft(X, A, n)\n\n\n\n\ncompute X \u00d7 A[1] \u00d7 A[2] \u00d7 ... \u00d7 A[n-1]\n\n\nArguments\n\n\n\n\nX\n  : tensor\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\nn\n  : Int64\n\n\n\n\nReturns\n\n\n\n\nLn\n : Array{Float64,2}\n\n\n\n\nsource\n\n\nefficient way to compute Ln based on the property X \u00d7 A[1] ... \u00d7A[n-1] \u00d7 A[n] = (X \u00d7 A[1] ... \u00d7A[n-1]) \u00d7 A[n]\n\n\n\n\nLnm1\n: Array{Float64,2} with size prod(I[1:n+1]) * R\n\n\nAnm1\n: Array{Array{Float64,2}} with size I[n+1] * R\n\n\nI\n   : Array{Int64}\n\n\nn\n   : Int64\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cp_gradient\n \n \nFunction\n.\n\n\ncp_gradient(Z, A, I, n)\n\n\n\n\nwhen dir=\"L\", it compute X \u00d7 A[-n] from X \u00d7 A[n+1]...A[N] when dir=\"R\", it compute X \u00d7 A[-n] from X \u00d7 A[1]...A[n-1]\n\n\nArguments\n\n\n\n\nZ\n: Array{Float64,2}\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\nI\n: Array{Int64,1}, dimension of tensor\n\n\nn\n: Int64.\n\n\n\n\nReturns\n\n\n\n\nG\n : Array{Float64,2} with size I[n] \u00d7 R\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.ttf\n \n \nFunction\n.\n\n\nttf(X, A, n)\n\n\n\n\nefficiet way computing tensor times all factor matrix except nth factor\n\n\nArguments\n\n\n\n\nX\n: tensor\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\nn\n: Int64, nth factor\n\n\n\n\nReturns\n\n\n\n\nG\n : Array{Float64,2} with size I[n] \u00d7 R\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.ttf_slow\n \n \nFunction\n.\n\n\nttf_slow(X, A, n)\n\n\n\n\ntensor times all factor matrix except nth factor the usage of this method is deprecated.\n\n\nArguments\n\n\n\n\nX\n: tensor\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\nn\n: Int64, nth factor\n\n\n\n\nReturns\n\n\n\n\nG\n : Array{Float64,2} with size I[n] \u00d7 R\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cpnormalize!\n \n \nFunction\n.\n\n\ncpnormalize!(lambda, A)\n\n\n\n\nnormalize factor matrix for CP decomposition and add weight to lambda\n\n\nArguments\n\n\n\n\nlambda\n: Array{Float64,1}, non-sorted weight\n\n\nA\n: Array{Array{Float64,2}}, Array of factor matrix\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.cp2tensor\n \n \nFunction\n.\n\n\ncp2tensor(X)\n\n\n\n\nconvert 4D cptensor to tensor\n\n\nsource\n\n\n#\n\n\nSAIGtensor.sidx2sub\n \n \nFunction\n.\n\n\nsidx2sub!(I, s)\n\n\n\n\nconvert linear indices to sub-index\n\n\nArguments\n\n\n\n\nI\n: Array{Int64,1}, dimensions\n\n\ns\n: Array{Int64,1}, linear index\n\n\n\n\nReturns\n\n\n\n\nsidx\n: Array{Tuple,1} with length(I)\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.randSpl!\n \n \nFunction\n.\n\n\nrandSpl!(Zs, Xs, A, T, ns, n)\n\n\n\n\nrandom sampling certain rows from Z[n]\n\n\nArguments\n\n\n\n\nZs\n: Array{Float64,2}, sampled Z[n]\n\n\nXst\n: Array{Array{Float64,2},1}, transpose of sampled X[n]\n\n\nA\n: Array{Array{Float64,2},1}, factor matrix\n\n\nns\n: Int64, number of samples\n\n\nn\n: Int64, specify which dimension\n\n\n\n\nsource\n\n\n#\n\n\nSAIGtensor.formXst!\n \n \nFunction\n.\n\n\nrandom sampling X[n]^T\n\n\nsource\n\n\n#\n\n\nSAIGtensor.formZs!\n \n \nFunction\n.\n\n\nrandom sampling Z[n]\n\n\nsource\n\n\n#\n\n\nSAIGtensor.updateAn!\n \n \nFunction\n.\n\n\nupdateAn!(A, AtA, Gn, n)\n\n\n\n\nupdate the nth entries in A and AtA\n\n\nArguments\n\n\n\n\nA\n  : Array{Array{Float64,2}}, Array of factor matrix\n\n\nAtA\n: Array{Array{Float64,2}}, Array of A[n]'\u00d7A[n]\n\n\nGn\n : Array{Float64,2}, Xn\u00d7(\u2297A[k]) for all k!=n\n\n\nn\n  : Int64, nth factor\n\n\n\n\nsource", 
            "title": "CP Tensor"
        }, 
        {
            "location": "/CPTensor/page1/#cp-tensor", 
            "text": "Some operations with respect to CP decomposition", 
            "title": "CP Tensor"
        }, 
        {
            "location": "/CPTensor/page1/#functions", 
            "text": "#  SAIGtensor.initCptensor     Function .  initCptensor(N, I, R, lambda, A)  create a cptensor object, right to test, it is a great work. fantastic packages.  Arguments   N : Int64, dimensions  I : Array{Int64}, size of each dimension  R : Int64, rank of tensor  lambda : eigenvalue of tensor  A : Array{Array{Float64,2}}, basis   Returns   X : CP tensor   Example  julia  N = 5; I = [23, 42, 13, 14, 17];\njulia  R = 10; lambda = rand(N);\njulia  A = Array{Array{Float64,2}}(N)\njulia  for m = 1 : N\njulia      A[m] = rand(I[m], R)\njulia  end\njulia  X = initCptensor(N, I, R, lambda, A);  source  #  SAIGtensor.cpnorm     Function .  l = cpnorm(X)  efficient way of computing the Frobenius norm of cp tensor  Arguments   X : cptensor   Returns   l : the frobenius norm of X   Example  julia  I = [21, 32, 43, 24]; R = 3;\njulia  N = length(I);\njulia  A = Array{Array{Float64}}(N)\njulia  for m = 1 : N\njulia      A[m] = rand(I[m], R)\njulia  end\njulia  X = initCptensor(N, I, R, lambda, A); l = cpnorm(X);  source  #  SAIGtensor.fitness     Function .  f = fitness(X, Y)  compute the relative fitness of lowrank cptensor to a full tensor  Arguments   X : cptensor  Y : tensor   Returns   f : relative fitness   source  #  SAIGtensor.cptfirst     Function .  cptfirst(I)  decide which factor should be update first, the updating order is ns, ns-1:-1:1, ns+1, ns+2:N  Arguments   I   : Array{Int64,1}, dimensions of tensor   source  #  SAIGtensor.cptRight     Function .  cptRight(X, A, n)  compute X \u00d7 A[n+1] \u00d7 A[n+2] \u00d7 ... \u00d7 A[N]  Arguments   X : tensor  A : Array{Array{Float64,2}}, Array of factor matrix  n : Int64   Returns   Rn  : Array{Float64,2}   source  When applied matrix, using the efficient way to compute them based on the property X \u00d7 A[n+1] \u00d7 A[n+2] \u00d7 ... \u00d7 A[N] = (X \u00d7 A[n+2] \u00d7 ... \u00d7 A[N]) \u00d7 A[n+1]   Rnp1 : Array{Float64,2} with size prod(I[1:n+1]) * R  Anp1 : Array{Array{Float64,2}} with size I[n+1] * R  I    : Array{Int64}  n    : Int64   source  #  SAIGtensor.cptLeft     Function .  cptLeft(X, A, n)  compute X \u00d7 A[1] \u00d7 A[2] \u00d7 ... \u00d7 A[n-1]  Arguments   X   : tensor  A : Array{Array{Float64,2}}, Array of factor matrix  n   : Int64   Returns   Ln  : Array{Float64,2}   source  efficient way to compute Ln based on the property X \u00d7 A[1] ... \u00d7A[n-1] \u00d7 A[n] = (X \u00d7 A[1] ... \u00d7A[n-1]) \u00d7 A[n]   Lnm1 : Array{Float64,2} with size prod(I[1:n+1]) * R  Anm1 : Array{Array{Float64,2}} with size I[n+1] * R  I    : Array{Int64}  n    : Int64   source  #  SAIGtensor.cp_gradient     Function .  cp_gradient(Z, A, I, n)  when dir=\"L\", it compute X \u00d7 A[-n] from X \u00d7 A[n+1]...A[N] when dir=\"R\", it compute X \u00d7 A[-n] from X \u00d7 A[1]...A[n-1]  Arguments   Z : Array{Float64,2}  A : Array{Array{Float64,2}}, Array of factor matrix  I : Array{Int64,1}, dimension of tensor  n : Int64.   Returns   G  : Array{Float64,2} with size I[n] \u00d7 R   source  #  SAIGtensor.ttf     Function .  ttf(X, A, n)  efficiet way computing tensor times all factor matrix except nth factor  Arguments   X : tensor  A : Array{Array{Float64,2}}, Array of factor matrix  n : Int64, nth factor   Returns   G  : Array{Float64,2} with size I[n] \u00d7 R   source  #  SAIGtensor.ttf_slow     Function .  ttf_slow(X, A, n)  tensor times all factor matrix except nth factor the usage of this method is deprecated.  Arguments   X : tensor  A : Array{Array{Float64,2}}, Array of factor matrix  n : Int64, nth factor   Returns   G  : Array{Float64,2} with size I[n] \u00d7 R   source  #  SAIGtensor.cpnormalize!     Function .  cpnormalize!(lambda, A)  normalize factor matrix for CP decomposition and add weight to lambda  Arguments   lambda : Array{Float64,1}, non-sorted weight  A : Array{Array{Float64,2}}, Array of factor matrix   source  #  SAIGtensor.cp2tensor     Function .  cp2tensor(X)  convert 4D cptensor to tensor  source  #  SAIGtensor.sidx2sub     Function .  sidx2sub!(I, s)  convert linear indices to sub-index  Arguments   I : Array{Int64,1}, dimensions  s : Array{Int64,1}, linear index   Returns   sidx : Array{Tuple,1} with length(I)   source  #  SAIGtensor.randSpl!     Function .  randSpl!(Zs, Xs, A, T, ns, n)  random sampling certain rows from Z[n]  Arguments   Zs : Array{Float64,2}, sampled Z[n]  Xst : Array{Array{Float64,2},1}, transpose of sampled X[n]  A : Array{Array{Float64,2},1}, factor matrix  ns : Int64, number of samples  n : Int64, specify which dimension   source  #  SAIGtensor.formXst!     Function .  random sampling X[n]^T  source  #  SAIGtensor.formZs!     Function .  random sampling Z[n]  source  #  SAIGtensor.updateAn!     Function .  updateAn!(A, AtA, Gn, n)  update the nth entries in A and AtA  Arguments   A   : Array{Array{Float64,2}}, Array of factor matrix  AtA : Array{Array{Float64,2}}, Array of A[n]'\u00d7A[n]  Gn  : Array{Float64,2}, Xn\u00d7(\u2297A[k]) for all k!=n  n   : Int64, nth factor   source", 
            "title": "Functions"
        }
    ]
}